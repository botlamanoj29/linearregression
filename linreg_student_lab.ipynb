{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Linear Regression & GLMs — Student Lab\n",
        "\n",
        "Complete all TODOs. Avoid sklearn for core parts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def check(name: str, cond: bool):\n",
        "    if not cond:\n",
        "        raise AssertionError(f'Failed: {name}')\n",
        "    print(f'OK: {name}')\n",
        "\n",
        "rng = np.random.default_rng(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 0 — Synthetic Dataset (with collinearity)\n",
        "We generate data where features can be highly correlated to motivate ridge."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(400, 5)\n",
            "(5,)\n",
            "(400,)\n",
            "OK: shapes\n",
            "corr(x0,x1)= 0.9985485848157019\n"
          ]
        }
      ],
      "source": [
        "def make_regression(n=400, d=5, noise=0.5, collinear=True):\n",
        "    X = rng.standard_normal((n, d))\n",
        "    print(X.shape)\n",
        "    if collinear and d >= 2:        \n",
        "        X[:, 1] = X[:, 0] * 0.95 + 0.05 * rng.standard_normal(n)\n",
        "        \n",
        "    w_true = rng.standard_normal(d)\n",
        "    print(w_true.shape)\n",
        "    y = X @ w_true + noise * rng.standard_normal(n)\n",
        "    print(y.shape)\n",
        "    return X, y, w_true\n",
        "\n",
        "X, y, w_true = make_regression()\n",
        "n, d = X.shape\n",
        "check('shapes', y.shape == (n,))\n",
        "print('corr(x0,x1)=', np.corrcoef(X[:,0], X[:,1])[0,1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 1 — OLS Closed Form\n",
        "\n",
        "### Task 1.1: Closed-form w_hat using solve\n",
        "\n",
        "# TODO: compute w_hat using solve on (X^T X) w = X^T y\n",
        "# HINT: `XtX = X.T@X`, `Xty = X.T@y`, `np.linalg.solve(XtX, Xty)`\n",
        "\n",
        "**Checkpoint:** Why is explicit inverse discouraged?\n",
        "Explicit inverse slow down the performance of the mathmetical operation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "OK: w_shape\n"
          ]
        }
      ],
      "source": [
        "# TODO\n",
        "XtX = X.T@X\n",
        "Xty = X.T@y\n",
        "w_hat = np.linalg.solve(XtX,Xty)\n",
        "\n",
        "check('w_shape', w_hat.shape == (d,))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Task 1.2: Evaluate fit + residuals\n",
        "Compute:\n",
        "- predictions y_pred\n",
        "- MSE\n",
        "- residual mean and std\n",
        "\n",
        "**Interview Angle:** What does a structured residual pattern imply (e.g., nonlinearity)?\n",
        "How much noise does the predication has.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "mse 0.0003388283091339161 resid_mean 0.01840728956511295 resid_std 0.4708303038387884\n",
            "OK: finite\n"
          ]
        }
      ],
      "source": [
        "# TODO\n",
        "y_pred = X @ w_hat\n",
        "mse = float(np.mean(y - y_pred) ** 2)\n",
        "resid = y_pred - y\n",
        "print('mse', mse, 'resid_mean', resid.mean(), 'resid_std', resid.std())\n",
        "check('finite', np.isfinite(mse))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 2 — Gradient Descent\n",
        "\n",
        "### Task 2.1: Implement MSE loss + gradient\n",
        "\n",
        "Loss = mean((Xw-y)^2), grad = (2/n) X^T(Xw-y)\n",
        "\n",
        "# TODO: implement `mse_loss_and_grad`\n",
        "\n",
        "**FAANG gotcha:** shapes and constants."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "OK: grad_shape\n",
            "OK: finite_loss\n"
          ]
        }
      ],
      "source": [
        "def mse_loss_and_grad(X, y, w):\n",
        "    # TODO\n",
        "    res = X @ w - y\n",
        "    loss = float(np.mean(res * res))\n",
        "    grad = (2/X.shape[0]) * X.T @ res\n",
        "    return loss, grad\n",
        "\n",
        "w0 = np.zeros(d)\n",
        "loss0, g0 = mse_loss_and_grad(X, y, w0)\n",
        "check('grad_shape', g0.shape == (d,))\n",
        "check('finite_loss', np.isfinite(loss0))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Task 2.2: Train with GD + compare to closed-form\n",
        "\n",
        "# TODO: implement a simple GD loop, track loss, and compare final weights to w_hat.\n",
        "\n",
        "**Checkpoint:** How does feature scaling affect GD?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(5,)\n",
            "(400, 5)\n",
            "[1.3079680318455007, 1.0859856075980903, 0.9097494347925305, 0.769773669812158, 0.6585500254815626, 0.5701335869476142, 0.49981572057306367, 0.44386569990997127, 0.399326561312197, 0.3638537653250385, 0.33558765404476804, 0.3130525971937641, 0.29507721934081527, 0.2807312830242583, 0.2692757348315747, 0.2601231567636425, 0.2528064452116741, 0.24695399748074096, 0.24227004690724788, 0.23851907264557748, 0.2355134352215397, 0.23310356664007703, 0.2311701841810513, 0.22961810789495843, 0.2283713494296068, 0.22736920907623598, 0.22656317268201334, 0.22591444338281902, 0.2253919773704756, 0.22497092002009272, 0.22463136016430063, 0.22435733729492888, 0.22413604993353797, 0.22395722407827415, 0.22381260908905248, 0.22369557507708307, 0.22360079118268794, 0.22352396834543817, 0.22346165352085343, 0.22341106495864843, 0.22336996027157674, 0.22333653070436063, 0.22330931634854623, 0.22328713811233902, 0.22326904310076692, 0.22325426073548066, 0.22324216748048667, 0.22323225846816772, 0.22322412466135977, 0.22321743445969525, 0.22321191887594818, 0.22320735958188664, 0.2232035792620332, 0.22320043382481672, 0.22319780610948683, 0.223195600798336, 0.22319374030079114, 0.22319216142164738, 0.22319081266237933, 0.22318965203389105, 0.2231886452826984, 0.2231877644515292, 0.22318698671059584, 0.22318629340808194, 0.22318566929827818, 0.22318510191376928, 0.22318458105450112, 0.22318409837173725, 0.223183647029097, 0.22318322142624736, 0.22318281697354864, 0.2231824299081631, 0.22318205714392303, 0.22318169614869926, 0.22318134484418542, 0.22318100152396148, 0.22318066478647225, 0.22318033348018113, 0.2231800066586677, 0.22317968354385123, 0.22317936349585654, 0.2231790459883129, 0.2231787305880991, 0.22317841693872673, 0.223178104746705, 0.22317779377034624, 0.22317748381057398, 0.22317717470337095, 0.22317686631357378, 0.22317655852977264, 0.22317625126011628, 0.22317594442886296, 0.22317563797354203, 0.22317533184261854, 0.22317502599357042, 0.22317472039130543, 0.22317441500685764, 0.2231741098163133, 0.2231738047999261, 0.22317349994138755, 0.22317319522722612, 0.22317289064631146, 0.22317258618944577, 0.22317228184902635, 0.22317197761876792, 0.22317167349347294, 0.22317136946884206, 0.2231710655413182, 0.22317076170795722, 0.22317045796632157, 0.2231701543143921, 0.22316985075049567, 0.223169547273245, 0.22316924388148907, 0.22316894057427233, 0.2231686373508007, 0.2231683342104136, 0.223168031152561, 0.22316772817678399, 0.22316742528269945, 0.22316712246998663, 0.22316681973837646, 0.22316651708764262, 0.22316621451759416, 0.22316591202806954, 0.22316560961893128, 0.22316530729006204, 0.22316500504136091, 0.22316470287274093, 0.2231644007841264, 0.22316409877545104, 0.2231637968466563, 0.22316349499769017, 0.22316319322850592, 0.2231628915390613, 0.22316258992931765, 0.22316228839923938, 0.2231619869487934, 0.2231616855779487, 0.223161384286676, 0.22316108307494745, 0.22316078194273636, 0.22316048089001708, 0.22316017991676473, 0.22315987902295514, 0.2231595782085646, 0.22315927747357, 0.22315897681794852, 0.22315867624167773, 0.22315837574473538, 0.22315807532709947, 0.22315777498874834, 0.2231574747296602, 0.2231571745498136, 0.22315687444918716, 0.22315657442775957, 0.22315627448550956, 0.2231559746224159, 0.22315567483845758, 0.22315537513361344, 0.2231550755078625, 0.22315477596118374, 0.2231544764935562, 0.22315417710495886, 0.22315387779537083, 0.22315357856477122, 0.22315327941313912, 0.22315298034045358, 0.2231526813466938, 0.223152382431839, 0.2231520835958682, 0.22315178483876058, 0.22315148616049535, 0.22315118756105165, 0.22315088904040878, 0.22315059059854575, 0.22315029223544194, 0.2231499939510764, 0.2231496957454285, 0.2231493976184774, 0.22314909957020224, 0.22314880160058237, 0.22314850370959696, 0.22314820589722534, 0.22314790816344668, 0.2231476105082403, 0.22314731293158538, 0.22314701543346127, 0.2231467180138472, 0.22314642067272245, 0.22314612341006637, 0.22314582622585818, 0.2231455291200772, 0.2231452320927027, 0.223144935143714, 0.22314463827309047, 0.22314434148081144, 0.22314404476685618, 0.22314374813120394, 0.22314345157383422, 0.2231431550947263, 0.22314285869385947, 0.22314256237121313, 0.22314226612676655, 0.2231419699604993, 0.2231416738723906, 0.22314137786241983, 0.22314108193056634, 0.22314078607680962, 0.22314049030112898, 0.2231401946035038, 0.22313989898391356, 0.22313960344233755, 0.22313930797875536, 0.22313901259314625, 0.22313871728548967, 0.22313842205576515, 0.22313812690395196, 0.2231378318300297, 0.22313753683397772, 0.22313724191577544, 0.22313694707540244, 0.22313665231283808, 0.22313635762806183, 0.2231360630210532, 0.22313576849179165, 0.22313547404025666, 0.2231351796664277, 0.22313488537028434, 0.223134591151806, 0.2231342970109722, 0.22313400294776248, 0.22313370896215634, 0.22313341505413323, 0.2231331212236728, 0.22313282747075447, 0.22313253379535788, 0.2231322401974625, 0.22313194667704786, 0.22313165323409362, 0.22313135986857924, 0.22313106658048426, 0.22313077336978837, 0.22313048023647097, 0.22313018718051184, 0.22312989420189044, 0.22312960130058634, 0.22312930847657927, 0.2231290157298487, 0.22312872306037435, 0.22312843046813566, 0.2231281379531124, 0.22312784551528417, 0.2231275531546305, 0.22312726087113116, 0.22312696866476572, 0.2231266765355138, 0.22312638448335512, 0.22312609250826923, 0.22312580061023593, 0.2231255087892348, 0.22312521704524546, 0.22312492537824766, 0.2231246337882211, 0.22312434227514547, 0.2231240508390004, 0.2231237594797656, 0.2231234681974208, 0.22312317699194564, 0.22312288586332002, 0.22312259481152347, 0.22312230383653578, 0.22312201293833667, 0.22312172211690587, 0.22312143137222315, 0.22312114070426822, 0.2231208501130209, 0.2231205595984609, 0.2231202691605679, 0.22311997879932186, 0.22311968851470232, 0.22311939830668925, 0.22311910817526232, 0.2231188181204014, 0.22311852814208627, 0.22311823824029667, 0.22311794841501245, 0.22311765866621344, 0.22311736899387938, 0.22311707939799014, 0.22311678987852557, 0.22311650043546538, 0.2231162110687896, 0.22311592177847792, 0.2231156325645103, 0.22311534342686648, 0.22311505436552642, 0.2231147653804699, 0.22311447647167681, 0.22311418763912705, 0.2231138988828005, 0.223113610202677, 0.2231133215987365, 0.22311303307095884, 0.22311274461932398, 0.2231124562438118, 0.22311216794440217, 0.22311187972107505, 0.22311159157381033, 0.223111303502588, 0.22311101550738793, 0.2231107275881901, 0.22311043974497444, 0.22311015197772083, 0.22310986428640933, 0.2231095766710198, 0.22310928913153233, 0.22310900166792677, 0.22310871428018317, 0.22310842696828145, 0.22310813973220167, 0.22310785257192373, 0.2231075654874277, 0.22310727847869352, 0.2231069915457013, 0.2231067046884309, 0.22310641790686248, 0.22310613120097592, 0.22310584457075136, 0.22310555801616885, 0.22310527153720833, 0.2231049851338499, 0.22310469880607364, 0.22310441255385954, 0.22310412637718768, 0.22310384027603813, 0.2231035542503909, 0.22310326830022625, 0.22310298242552404, 0.22310269662626447, 0.22310241090242755, 0.2231021252539935, 0.22310183968094235, 0.22310155418325422, 0.2231012687609092, 0.22310098341388737, 0.22310069814216898, 0.22310041294573402, 0.22310012782456273, 0.2230998427786352, 0.22309955780793153, 0.22309927291243198, 0.2230989880921166, 0.22309870334696552, 0.223098418676959, 0.22309813408207724, 0.22309784956230028, 0.22309756511760842, 0.22309728074798177, 0.22309699645340053, 0.22309671223384497, 0.22309642808929517, 0.22309614401973143, 0.22309586002513393, 0.2230955761054829, 0.22309529226075853, 0.22309500849094102, 0.22309472479601072, 0.2230944411759478, 0.2230941576307324, 0.22309387416034496, 0.22309359076476562, 0.22309330744397463, 0.22309302419795224, 0.22309274102667878, 0.22309245793013452, 0.22309217490829977, 0.2230918919611547, 0.22309160908867967, 0.22309132629085493, 0.22309104356766088, 0.22309076091907776, 0.22309047834508583, 0.2230901958456655, 0.223089913420797, 0.22308963107046076, 0.223089348794637, 0.22308906659330618, 0.22308878446644848, 0.22308850241404443, 0.2230882204360742, 0.22308793853251832, 0.22308765670335698, 0.2230873749485707, 0.22308709326813983, 0.22308681166204458, 0.22308653013026558, 0.22308624867278304, 0.22308596728957739, 0.22308568598062906, 0.22308540474591843, 0.22308512358542593, 0.223084842499132, 0.22308456148701697, 0.22308428054906138, 0.22308399968524553, 0.22308371889554995, 0.22308343817995502, 0.22308315753844124, 0.22308287697098905, 0.22308259647757886, 0.22308231605819118, 0.22308203571280644, 0.2230817554414051, 0.2230814752439677, 0.2230811951204747, 0.22308091507090647, 0.22308063509524367, 0.2230803551934667, 0.22308007536555613, 0.2230797956114924, 0.22307951593125608, 0.22307923632482762, 0.22307895679218756, 0.2230786773333165, 0.22307839794819478, 0.2230781186368032, 0.2230778393991221, 0.22307756023513214, 0.22307728114481382, 0.22307700212814777, 0.2230767231851144, 0.22307644431569443, 0.22307616551986834, 0.22307588679761675, 0.22307560814892027, 0.22307532957375945, 0.2230750510721149, 0.22307477264396716, 0.22307449428929693, 0.22307421600808475, 0.22307393780031126, 0.2230736596659571, 0.22307338160500287, 0.22307310361742919, 0.2230728257032167, 0.22307254786234604, 0.22307227009479788, 0.22307199240055287, 0.2230717147795916, 0.2230714372318948, 0.2230711597574431, 0.22307088235621722, 0.22307060502819775, 0.22307032777336544, 0.22307005059170096, 0.22306977348318502, 0.22306949644779828, 0.2230692194855214, 0.2230689425963352, 0.22306866578022028, 0.22306838903715748, 0.2230681123671274, 0.22306783577011083, 0.22306755924608843, 0.22306728279504107, 0.22306700641694938, 0.22306673011179418, 0.22306645387955612, 0.22306617772021606, 0.22306590163375473, 0.22306562562015286, 0.2230653496793913, 0.22306507381145074, 0.22306479801631202, 0.2230645222939559, 0.2230642466443632, 0.2230639710675147, 0.2230636955633912, 0.22306342013197353, 0.22306314477324243, 0.22306286948717882, 0.22306259427376346, 0.22306231913297714, 0.22306204406480085, 0.22306176906921527, 0.2230614941462013, 0.2230612192957398, 0.22306094451781155, 0.22306066981239756, 0.2230603951794785, 0.22306012061903543, 0.22305984613104912, 0.22305957171550042, 0.2230592973723703, 0.22305902310163958, 0.22305874890328917, 0.22305847477729998, 0.22305820072365293, 0.2230579267423289]\n",
            "[ 0.07255782 -0.01345896 -0.06774265 -0.44861741  0.89645561]\n",
            "final_loss 0.2230579267423289\n",
            "||w_gd-w_hat|| 0.8867587892417692\n",
            "OK: loss_decreases\n"
          ]
        }
      ],
      "source": [
        "def train_gd(X, y, lr=0.05, steps=500):\n",
        "    # TODO\n",
        "    w = np.zeros(X.shape[1])\n",
        "    Losses = []\n",
        "    print(w.shape)\n",
        "    print(X.shape)\n",
        "    for i in range(steps):\n",
        "        loss, grad = mse_loss_and_grad(X, y , w)\n",
        "        w = w - lr * grad\n",
        "        Losses.append(loss)\n",
        "    print(Losses)\n",
        "    print(w)\n",
        "    return w, Losses\n",
        "    \n",
        "w_gd, losses = train_gd(X, y, lr=0.05, steps=500)\n",
        "print('final_loss', losses[-1])\n",
        "print('||w_gd-w_hat||', np.linalg.norm(w_gd - w_hat))\n",
        "check('loss_decreases', losses[-1] <= losses[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 3 — Ridge Regression (L2)\n",
        "\n",
        "### Task 3.1: Ridge closed-form\n",
        "w = (X^T X + λI)^{-1} X^T y\n",
        "\n",
        "# TODO: implement ridge_solve\n",
        "\n",
        "**Interview Angle:** Why does ridge help under collinearity?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "OK: ridge_shape\n"
          ]
        }
      ],
      "source": [
        "def ridge_solve(X, y, lam):\n",
        "    # TODO\n",
        "    d = X.shape[1]\n",
        "    return np.linalg.solve(X.T @ X + lam * np.eye(d) , X.T @ y)\n",
        "\n",
        "w_ridge = ridge_solve(X, y, lam=1.0)\n",
        "check('ridge_shape', w_ridge.shape == (d,))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Task 3.2: Bias/variance demo with train/test split\n",
        "\n",
        "# TODO: split into train/test and compare MSE for multiple lambdas.\n",
        "\n",
        "**Checkpoint:** why can test error improve even when train error worsens?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "lam, train_mse, test_mse\n",
            "(0.0, 0.22434062830819776, 0.22015868447129303)\n",
            "(0.1, 0.22436070870186797, 0.22054748124275209)\n",
            "(1.0, 0.22456604110932607, 0.22130065084795897)\n",
            "(10.0, 0.22589656843495562, 0.218772678805243)\n"
          ]
        }
      ],
      "source": [
        "# TODO\n",
        "idx = rng.permutation(n)\n",
        "train = idx[: int(0.7*n)]\n",
        "test = idx[int(0.7*n):]\n",
        "Xtr, ytr = X[train], y[train]\n",
        "Xte, yte = X[test], y[test]\n",
        "\n",
        "\n",
        "lams = [0.0, 0.1, 1.0, 10.0]\n",
        "results = []\n",
        "for lam in lams:\n",
        "    w = ridge_solve(Xtr, ytr, lam=lam) if lam > 0 else np.linalg.solve(Xtr.T@Xtr, Xtr.T@ytr)\n",
        "    tr_mse = np.mean((Xtr@w - ytr)**2)\n",
        "    te_mse = np.mean((Xte@w - yte)**2)\n",
        "    results.append((lam, tr_mse, te_mse))\n",
        "print('lam, train_mse, test_mse')\n",
        "for r in results:\n",
        "    print(r)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 4 — GLM Intuition\n",
        "\n",
        "### Task 4.1: Match tasks to (distribution, link)\n",
        "Fill in a table for:\n",
        "- regression\n",
        "- binary classification\n",
        "- count prediction\n",
        "\n",
        "**Explain:** what changes when you go from OLS to a GLM?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "| Problem | Target type | Distribution | Link | Loss |\n",
        "|---|---|---|---|---|\n",
        "| House price | continuous | ? | ? | ? |\n",
        "| Fraud | binary | ? | ? | ? |\n",
        "| Clicks per user | count | ? | ? | ? |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Submission Checklist\n",
        "- All TODOs completed\n",
        "- Train/test results shown for ridge\n",
        "- Short answers to checkpoint questions\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
